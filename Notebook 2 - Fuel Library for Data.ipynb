{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuel Library for Data\n",
    "\n",
    "Notebook #1 explored the data that we were dealing with. This notebook utilizes the [Fuel library](https://github.com/mila-udem/fuel), which wraps data for machine learning pipelines, and the [lfw_fuel library](https://github.com/dribnet/lfw_fuel), which extends the Fuel library to the LFW dataset.\n",
    "\n",
    "This enables us to load image data and convert it into X and Y training/testing vectors in one call, like this:\n",
    "\n",
    "```\n",
    "(X_train, y_train), (X_test, y_test) = lfw.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = lfw.load_data(\"funneled\")\n",
    "(X_train, y_train), (X_test, y_test) = lfw.load_data(\"deepfunneled\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from scipy.misc import imresize \n",
    "\n",
    "from lfw_fuel import lfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 2\n",
    "nb_epoch = 12\n",
    "feature_width = 32\n",
    "feature_height = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cropImage(im):\n",
    "    im2 = np.dstack(im).astype(np.uint8)\n",
    "    # return centered 128x128 from original 250x250 (40% of area)\n",
    "    newim = im2[61:189, 61:189]\n",
    "    sized1 = imresize(newim[:,:,0:3], (feature_width, feature_height), interp=\"bicubic\", mode=\"RGB\")\n",
    "    sized2 = imresize(newim[:,:,3:6], (feature_width, feature_height), interp=\"bicubic\", mode=\"RGB\")\n",
    "    return np.asarray([sized1[:,:,0], sized1[:,:,1], sized1[:,:,2], sized2[:,:,0], sized2[:,:,1], sized2[:,:,2]])\n",
    "\n",
    "a=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = lfw.load_data(\"deepfunneled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Crop features\n",
    "X_train = np.asarray(list(map(cropImage, X_train)))\n",
    "X_test = np.asarray(list(map(cropImage, X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200 train samples, 6 channels, 32x32\n",
      "1000  test samples, 6 channels, 32x32\n"
     ]
    }
   ],
   "source": [
    "# print shape of data\n",
    "print(\"{1} train samples, {2} channel{0}, {3}x{4}\".format(\"\" if X_train.shape[1] == 1 else \"s\", *X_train.shape))\n",
    "print(\"{1}  test samples, {2} channel{0}, {3}x{4}\".format(\"\" if X_test.shape[1] == 1 else \"s\", *X_test.shape))\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In general, it is a good idea to use normalized data.\n",
    "# We compute these for now, but don't use them below. Save for later.\n",
    "X_train_float = X_train.astype('float32')\n",
    "X_train_float /= 255\n",
    "\n",
    "X_test_float = X_test.astype('float32')\n",
    "X_test_float /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "Now that we've used the fuel library to load our data, we're ready to train our first neural network. We will use a convolutional neural network, useful for image recognition and classification tasks.\n",
    "\n",
    "(Useful links on [convolutional neural networks](http://machinelearningmastery.com/crash-course-convolutional-neural-networks/) and [object recognition](http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/) from Dr. Jason Brownlee, very helpful in understanding these concepts. [Book chaper](http://neuralnetworksanddeeplearning.com/chap6.html) from Michael Nielsen also useful.)\n",
    "\n",
    "Convolutional neural networks use three types of layers:\n",
    "* Convolutional layers\n",
    "* Pooling layers\n",
    "* Fully connected layers\n",
    "\n",
    "Convolutional layers are comprised of filters and feature maps. \n",
    "* Filters are the neurons of the layer, which have inputs, weights, and outputs. Input size is a fixed patch. For input convolutional layers, these patches are pixels striaght from the image. Deeper in the neural network, the patches are the outputs from prior layers.\n",
    "* Feature maps are how outputs from one layer are connected to inputs at the next layer. \n",
    "* Padding is necessitated by the fact that one layer's output sie may not be cleanly divisible by the filter patch at the next layer. Zero padding can be used to keep the neural net from reading off the edge of the image.\n",
    "\n",
    "Pooling layers down-sample a given layer.\n",
    "* Pooling acts as a compression or dimensionality reduction, condensing the features learned in prior layers to the most important ones. Their input size is often much smaller than the convolutional layer they are connected to. \n",
    "* These create their own feature maps (how outputs from one layer are connected to inputs at the next layer), often using an average or maximum function.\n",
    "\n",
    "Fully connected layers are used to combine the various extracted features.\n",
    "* Fully connected layers create a non-linear combination of all incoming features.\n",
    "* Activation functions at the connected layer is often a softmax or non-linear function. \n",
    "* These can be thought of as predicting the probability of a particular class or classification.\n",
    "\n",
    "The general architecture of a convolutional neural network is:\n",
    "* Convolution\n",
    "* Convolution\n",
    "* Pool\n",
    "* Dropout\n",
    "* Flatten\n",
    "* Dense\n",
    "* Dropout\n",
    "* Dense\n",
    "\n",
    "This can take other forms, like:\n",
    "* Convolution\n",
    "* Dropout\n",
    "* Convolution\n",
    "* Pool\n",
    "* Flatten\n",
    "* Dense/Fully connected\n",
    "* Dropout\n",
    "* Dense/Fully connected (n_classes)\n",
    "\n",
    "Optionally, to add more convolution layers,\n",
    "* Convolution (32 feature maps)\n",
    "* Dropout\n",
    "* Convolution (32 feature maps)\n",
    "* Pool\n",
    "* Convolution (64 feature maps)\n",
    "* Dropout\n",
    "* Convolution (64 feature maps)\n",
    "* Pool\n",
    "* Convolution (128 feature maps)\n",
    "* Dropout\n",
    "* Convolution (128 feature maps)\n",
    "* Pool\n",
    "* Flatten\n",
    "* Dropout\n",
    "* Dense/Fully connected (1024)\n",
    "* Dropout\n",
    "* Dense/Fully connected (512)\n",
    "* Dropout\n",
    "* Dense/Fully connected (n_classes)\n",
    "\n",
    "In Keras, it takes a little bit of effort to understand how to get all of the layers to line up. The comments in the neural network constructed below should explain what's happening at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2200, 6, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train_float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_78 (Conv2D)           (None, 4, 30, 32)         9248      \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 2, 28, 32)         9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 1, 14, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 1, 14, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 128)               57472     \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 76,226\n",
      "Trainable params: 76,226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "modelA = Sequential()\n",
    "\n",
    "# Convolutional input layer, \n",
    "# 32 feature maps with a size of 3×3,\n",
    "# and a weight constraint of max norm set to 3.\n",
    "# Image sizes are (6, 32, 32) - 6 color channels, 32 x 32 pixels\n",
    "modelA.add(Conv2D(32, (3, 3), input_shape=(6, 32, 32),\n",
    "                 padding='valid', activation='relu'))\n",
    "\n",
    "# Convolutional layer, \n",
    "# 32 feature maps with a size of 3×3, \n",
    "# a rectifier activation function \n",
    "# and a weight constraint of max norm set to 3.\n",
    "modelA.add(Conv2D(32, (3, 3), input_shape=(6, 32, 32),\n",
    "                  padding='valid', activation='relu'))\n",
    "\n",
    "# Max Pool layer with size 2×2.\n",
    "modelA.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Dropout set to 20%\n",
    "modelA.add(Dropout(0.2))\n",
    "\n",
    "# Flatten layer.\n",
    "modelA.add(Flatten())\n",
    "\n",
    "# Fully connected layer with 128 units and a rectifier activation function.\n",
    "modelA.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Dropout set to 50%.\n",
    "modelA.add(Dropout(0.5))\n",
    "\n",
    "# Fully connected output layer with 2 units (Y/N) \n",
    "# and a softmax activation function.\n",
    "modelA.add(Dense(2,activation='softmax'))\n",
    "\n",
    "print(modelA.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model:\n",
    "# A logarithmic loss function is used with \n",
    "# stochastic gradient descent optimization algorithm\n",
    "# configured with a large momentum and weight decay \n",
    "# starting with a learning rate of 0.01.\n",
    "epochs = 12\n",
    "lrate = 0.1\n",
    "decay = lrate/epochs\n",
    "batch_size = 32\n",
    "\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "modelA.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2200 samples, validate on 1000 samples\n",
      "Epoch 1/12\n",
      "2200/2200 [==============================] - 2s - loss: 0.6818 - binary_accuracy: 0.5341 - val_loss: 0.7014 - val_binary_accuracy: 0.4930\n",
      "Epoch 2/12\n",
      "2200/2200 [==============================] - 1s - loss: 0.6735 - binary_accuracy: 0.5482 - val_loss: 0.7120 - val_binary_accuracy: 0.5040\n",
      "Epoch 3/12\n",
      "2200/2200 [==============================] - 1s - loss: 0.6530 - binary_accuracy: 0.5568 - val_loss: 0.7554 - val_binary_accuracy: 0.4990\n",
      "Epoch 4/12\n",
      "2200/2200 [==============================] - 1s - loss: 0.6383 - binary_accuracy: 0.5709 - val_loss: 0.7486 - val_binary_accuracy: 0.5170\n",
      "Epoch 5/12\n",
      "2200/2200 [==============================] - 1s - loss: 0.6214 - binary_accuracy: 0.5855 - val_loss: 0.7702 - val_binary_accuracy: 0.4990\n",
      "Epoch 6/12\n",
      "2200/2200 [==============================] - 1s - loss: 0.6043 - binary_accuracy: 0.6241 - val_loss: 0.7734 - val_binary_accuracy: 0.5190\n",
      "Epoch 7/12\n",
      "2200/2200 [==============================] - 1s - loss: 0.5960 - binary_accuracy: 0.6109 - val_loss: 0.8078 - val_binary_accuracy: 0.4910\n",
      "Epoch 8/12\n",
      "2200/2200 [==============================] - 1s - loss: 0.5801 - binary_accuracy: 0.6068 - val_loss: 0.8110 - val_binary_accuracy: 0.4910\n",
      "Epoch 9/12\n",
      "2200/2200 [==============================] - 1s - loss: 0.5689 - binary_accuracy: 0.6345 - val_loss: 0.8336 - val_binary_accuracy: 0.4990\n",
      "Epoch 10/12\n",
      "2200/2200 [==============================] - 1s - loss: 0.5700 - binary_accuracy: 0.6305 - val_loss: 0.8312 - val_binary_accuracy: 0.4930\n",
      "Epoch 11/12\n",
      "2200/2200 [==============================] - 1s - loss: 0.5622 - binary_accuracy: 0.6405 - val_loss: 0.8603 - val_binary_accuracy: 0.4870\n",
      "Epoch 12/12\n",
      "2200/2200 [==============================] - 1s - loss: 0.5496 - binary_accuracy: 0.6473 - val_loss: 0.8364 - val_binary_accuracy: 0.5020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12672eac8>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelA.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Model A (12 epochs):\n",
      "Test accuracy: 50.200000%\n"
     ]
    }
   ],
   "source": [
    "# lower-case y_test is not encoded, CAPITAL Y_test is hot-encoded\n",
    "# use CAPITAL-Y_test!!\n",
    "score = modelA.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print(\"-\"*40)\n",
    "print(\"Model A (12 epochs):\")\n",
    "print('Test accuracy: {0:%}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What A Terrible Model\n",
    "\n",
    "It is a bit of a disappointment to get such an awful result from our neural network, but remember, this was just our first pass. Some other things we can work on:\n",
    "* Use normalized data\n",
    "* Add additional convolutional layers to the neural network\n",
    "* Train for more epochs\n",
    "* Use larger neural network layers (more neurons per network)\n",
    "\n",
    "Let's run Model B, which is the same as Model A, but for 120 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_82 (Conv2D)           (None, 4, 30, 32)         9248      \n",
      "_________________________________________________________________\n",
      "conv2d_83 (Conv2D)           (None, 2, 28, 32)         9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 1, 14, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 1, 14, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 128)               57472     \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 76,226\n",
      "Trainable params: 76,226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "modelB = Sequential()\n",
    "\n",
    "# Convolutional input layer, \n",
    "# 32 feature maps with a size of 3×3,\n",
    "# and a weight constraint of max norm set to 3.\n",
    "# Image sizes are (6, 32, 32) - 6 color channels, 32 x 32 pixels\n",
    "modelB.add(Conv2D(32, (3, 3), input_shape=(6, 32, 32),\n",
    "                 padding='valid', activation='relu'))\n",
    "\n",
    "# Convolutional layer, \n",
    "# 32 feature maps with a size of 3×3, \n",
    "# a rectifier activation function \n",
    "# and a weight constraint of max norm set to 3.\n",
    "modelB.add(Conv2D(32, (3, 3), input_shape=(6, 32, 32),\n",
    "                  padding='valid', activation='relu'))\n",
    "\n",
    "# Max Pool layer with size 2×2.\n",
    "modelB.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Dropout set to 20%\n",
    "modelB.add(Dropout(0.2))\n",
    "\n",
    "# Flatten layer.\n",
    "modelB.add(Flatten())\n",
    "\n",
    "# Fully connected layer with 128 units and a rectifier activation function.\n",
    "modelB.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Dropout set to 50%.\n",
    "modelB.add(Dropout(0.5))\n",
    "\n",
    "# Fully connected output layer with 2 units (Y/N) \n",
    "# and a softmax activation function.\n",
    "modelB.add(Dense(2,activation='softmax'))\n",
    "\n",
    "# Compile model:\n",
    "# A logarithmic loss function is used with \n",
    "# stochastic gradient descent optimization algorithm\n",
    "# configured with a large momentum and weight decay \n",
    "# starting with a learning rate of 0.01.\n",
    "epochs = 120\n",
    "lrate = 0.1\n",
    "decay = lrate/epochs\n",
    "batch_size = 32\n",
    "\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "modelB.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['binary_accuracy'])\n",
    "\n",
    "print(modelB.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b26bb38>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelB.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Model B (120 epochs):\n",
      "Test accuracy: 51.000000%\n"
     ]
    }
   ],
   "source": [
    "# lower-case y_test is not encoded, CAPITAL Y_test is hot-encoded\n",
    "# use CAPITAL-Y_test!!\n",
    "score = modelB.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print(\"-\"*40)\n",
    "print(\"Model B (120 epochs):\")\n",
    "print('Test accuracy: {0:%}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Yikes! That is still a pretty bad result. In the next notebook we'll work on building out the neural network and try to improve on these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
